{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: \n",
    "\n",
    "A1.Download and install Gutenberg corpus tool to your Jupyter Notebook. Project Gutenberg contains some 25,000 free electronic books, hosted at http://www.gutenberg.org/. We can install the NLTK package, then use the Gutenberg corpus in it.  Can be installed by running the following in computer terminal: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in ./opt/anaconda3/lib/python3.7/site-packages (3.4.5)\n",
      "Requirement already satisfied: six in ./opt/anaconda3/lib/python3.7/site-packages (from nltk) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B.Download Gutenberg corpus tool in NLTK package by e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/animeshsrivastava/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk.corpus\n",
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "from nltk.corpus import inaugural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import ConditionalFreqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C.Use the texts in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D.Create a table displaying relative frequencies with which “modals” (can, could, may, might, will, would and should) are used in all texts provided in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2621613"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = []\n",
    "for fileid in nltk.corpus.gutenberg.fileids():\n",
    "    for w in nltk.corpus.gutenberg.words(fileid):\n",
    "        words.append(w)\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  will  would  could should    may    can  might \n",
      "  7130   3932   3528   2496   2435   2163   1938 \n"
     ]
    }
   ],
   "source": [
    "modals = ['can','could','may','might','will','would','should']\n",
    "filtered_words = [word for word in words if word in modals]\n",
    "FreqDist(filtered_words).tabulate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E.For two modals with the largest span of relative frequencies (most used minus least used), select a text which uses it the most and the text that uses it the least. Compare usage in both texts by examining the relative frequencies of those modals in two texts. Try to suggest an explanation why  those words used differently in the two texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        might  will \n",
      "        austen-emma.txt   322   559 \n",
      "  austen-persuasion.txt   166   162 \n",
      "       austen-sense.txt   215   354 \n",
      "          bible-kjv.txt   475  3807 \n",
      "        blake-poems.txt     2     3 \n",
      "     bryant-stories.txt    23   144 \n",
      "burgess-busterbrown.txt    17    19 \n",
      "      carroll-alice.txt    28    24 \n",
      "    chesterton-ball.txt    69   198 \n",
      "   chesterton-brown.txt    71   111 \n",
      "chesterton-thursday.txt    71   109 \n",
      "  edgeworth-parents.txt   127   517 \n",
      " melville-moby_dick.txt   183   379 \n",
      "    milton-paradise.txt    98   161 \n",
      " shakespeare-caesar.txt    12   129 \n",
      " shakespeare-hamlet.txt    28   131 \n",
      "shakespeare-macbeth.txt     5    62 \n",
      "     whitman-leaves.txt    26   261 \n"
     ]
    }
   ],
   "source": [
    "rfd= nltk.ConditionalFreqDist(\n",
    "    (files,word)\n",
    "    for files in gutenberg.fileids()\n",
    "    for word in gutenberg.words(files))\n",
    "modals = ['might','will']\n",
    "rfd.tabulate(conditions=gutenberg.fileids(), samples=modals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the above result the two modals with the largest relative frequencies are - \"will\" and \"might\". In both the modals, bible-kjv.txt uses the most with relative frequencies of 3807 and 475. In both the modals, blake-poems.txt uses the least with relative frequencies of 3 and 2. It is very clear that bible wil have the most usage of these modals as they connates occurance of any event that is going to happen. Bible is also written mostly in future tense. These modals are least used in blake poems because they mostly signify romanticism and the writing style contains less modals due to rhyming nature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: \n",
    "\n",
    "A2.In the Inaugural corpus, see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package inaugural to\n",
      "[nltk_data]     /Users/animeshsrivastava/nltk_data...\n",
      "[nltk_data]   Package inaugural is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('inaugural')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B.Chose Kennedy’s speech, using e.g. this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_words = nltk.corpus.inaugural.words('1961-Kennedy.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C.Identify the 10 most frequently used long words (words longer than 7 characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_char = ([w.lower() for w in np_words if len(w) > 7])\n",
    "\n",
    "# Frequency distribution\n",
    "f_dist = FreqDist(filt_char)\n",
    "freq = f_dist.most_common(10)\n",
    "\n",
    "# Creaty empty arrays\n",
    "np_1 = []\n",
    "np_2 = []\n",
    "\n",
    "# Seperate words\n",
    "for i in range(10):\n",
    "    np_1.append(freq[i][0])\n",
    "\n",
    "for i in range(10):\n",
    "    np_2.append(freq[i][1])\n",
    "\n",
    "# Create dataframe\n",
    "df_freq = pd.DataFrame({'Top 10':np_1, 'n': np_2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Top 10</th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>citizens</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>president</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>americans</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>generation</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>forebears</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>revolution</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>committed</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>powerful</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>supporting</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>themselves</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Top 10  n\n",
       "0    citizens  5\n",
       "1   president  4\n",
       "2   americans  4\n",
       "3  generation  3\n",
       "4   forebears  2\n",
       "5  revolution  2\n",
       "6   committed  2\n",
       "7    powerful  2\n",
       "8  supporting  2\n",
       "9  themselves  2"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D.Which one of those 10 words has the largest number of synonyms? Use WordNet as a helper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word:  citizens\n",
      "Synonymns: \n",
      "   ['citizen']\n",
      "Word:  president\n",
      "Synonymns: \n",
      "   ['president']\n",
      "   ['President_of_the_United_States', 'United_States_President', 'President', 'Chief_Executive']\n",
      "   ['president']\n",
      "   ['president', 'chairman', 'chairwoman', 'chair', 'chairperson']\n",
      "   ['president', 'prexy']\n",
      "   ['President_of_the_United_States', 'President', 'Chief_Executive']\n",
      "Word:  americans\n",
      "Synonymns: \n",
      "   ['American']\n",
      "   ['American_English', 'American_language', 'American']\n",
      "   ['American']\n",
      "Word:  generation\n",
      "Synonymns: \n",
      "   ['coevals', 'contemporaries', 'generation']\n",
      "   ['generation']\n",
      "   ['generation']\n",
      "   ['generation']\n",
      "   ['genesis', 'generation']\n",
      "   ['generation']\n",
      "   ['generation', 'multiplication', 'propagation']\n",
      "Word:  forebears\n",
      "Synonymns: \n",
      "   ['forebear', 'forbear']\n",
      "Word:  revolution\n",
      "Synonymns: \n",
      "   ['revolution']\n",
      "   ['revolution']\n",
      "   ['rotation', 'revolution', 'gyration']\n",
      "Word:  committed\n",
      "Synonymns: \n",
      "   ['perpetrate', 'commit', 'pull']\n",
      "   ['give', 'dedicate', 'consecrate', 'commit', 'devote']\n",
      "   ['commit', 'institutionalize', 'institutionalise', 'send', 'charge']\n",
      "   ['entrust', 'intrust', 'trust', 'confide', 'commit']\n",
      "   ['invest', 'put', 'commit', 'place']\n",
      "   ['commit', 'practice']\n",
      "   ['committed']\n",
      "   ['attached', 'committed']\n",
      "Word:  powerful\n",
      "Synonymns: \n",
      "   ['powerful']\n",
      "   ['knock-down', 'powerful']\n",
      "   ['potent', 'powerful']\n",
      "   ['brawny', 'hefty', 'muscular', 'powerful', 'sinewy']\n",
      "   ['herculean', 'powerful']\n",
      "   ['mighty', 'mightily', 'powerful', 'right']\n",
      "Word:  supporting\n",
      "Synonymns: \n",
      "   ['support', 'supporting']\n",
      "   ['support', 'back_up']\n",
      "   ['support']\n",
      "   ['back', 'endorse', 'indorse', 'plump_for', 'plunk_for', 'support']\n",
      "   ['hold', 'support', 'sustain', 'hold_up']\n",
      "   ['confirm', 'corroborate', 'sustain', 'substantiate', 'support', 'affirm']\n",
      "   ['subscribe', 'support']\n",
      "   ['corroborate', 'underpin', 'bear_out', 'support']\n",
      "   ['defend', 'support', 'fend_for']\n",
      "   ['support']\n",
      "   ['patronize', 'patronise', 'patronage', 'support', 'keep_going']\n",
      "   ['digest', 'endure', 'stick_out', 'stomach', 'bear', 'stand', 'tolerate', 'support', 'brook', 'abide', 'suffer', 'put_up']\n",
      "   ['encouraging', 'supporting']\n",
      "   ['load-bearing', 'supporting']\n",
      "Word:  themselves\n",
      "Synonymns: \n"
     ]
    }
   ],
   "source": [
    "np_syn = []\n",
    "\n",
    "# Loop trough distribution\n",
    "for i in f_dist.most_common(10):\n",
    "    # Print word\n",
    "    print( 'Word: ', i[0])\n",
    "    count=0\n",
    "    \n",
    "    # Print synonym\n",
    "    print ('Synonymns: ')\n",
    "    for j in wn.synsets(i[0]):\n",
    "        print ('  ',j.lemma_names())\n",
    "        count+=len(j.lemma_names())\n",
    "    np_syn.append([i[0], count])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creaty empty arrays\n",
    "np_1 = []\n",
    "np_2 = []\n",
    "\n",
    "# Get synonyms\n",
    "for i in range(10):\n",
    "    np_1.append(np_syn[i][0])\n",
    "\n",
    "for j in range(10):\n",
    "    np_2.append(np_syn[j][1])\n",
    "\n",
    "# Create dataframe\n",
    "df_syn = pd.DataFrame({'Top 10 synonyms':np_1, 'n':np_2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E.List all synonyms for the 10 most frequently used words. Which one of those 10 words has the largest number of hyponyms? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Top 10 synonyms</th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>supporting</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>committed</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>president</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>powerful</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>generation</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>americans</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>revolution</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>forebears</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>citizens</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>themselves</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Top 10 synonyms   n\n",
       "8      supporting  52\n",
       "6       committed  27\n",
       "1       president  16\n",
       "7        powerful  16\n",
       "3      generation  12\n",
       "2       americans   5\n",
       "5      revolution   5\n",
       "4       forebears   2\n",
       "0        citizens   1\n",
       "9      themselves   0"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_syn.sort_values(by=\"n\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word:  citizens\n",
      "Hyponyms: \n",
      "Synset('citizen.n.01')\n",
      "[Synset('active_citizen.n.01'), Synset('civilian.n.01'), Synset('freeman.n.01'), Synset('private_citizen.n.01'), Synset('repatriate.n.01'), Synset('thane.n.02'), Synset('voter.n.01')]\n",
      "Word:  president\n",
      "Hyponyms: \n",
      "Synset('president.n.01')\n",
      "[]\n",
      "Synset('president_of_the_united_states.n.01')\n",
      "[]\n",
      "Synset('president.n.03')\n",
      "[Synset('ex-president.n.01')]\n",
      "Synset('president.n.04')\n",
      "[Synset('kalon_tripa.n.01'), Synset('vice_chairman.n.01')]\n",
      "Synset('president.n.05')\n",
      "[]\n",
      "Synset('president_of_the_united_states.n.02')\n",
      "[]\n",
      "Word:  americans\n",
      "Hyponyms: \n",
      "Synset('american.n.01')\n",
      "[Synset('african-american.n.01'), Synset('alabaman.n.01'), Synset('alaskan.n.01'), Synset('anglo-american.n.01'), Synset('appalachian.n.01'), Synset('arizonan.n.01'), Synset('arkansan.n.01'), Synset('asian_american.n.01'), Synset('bay_stater.n.01'), Synset('bostonian.n.01'), Synset('californian.n.01'), Synset('carolinian.n.01'), Synset('coloradan.n.01'), Synset('connecticuter.n.01'), Synset('creole.n.02'), Synset('delawarean.n.01'), Synset('floridian.n.01'), Synset('franco-american.n.01'), Synset('georgian.n.01'), Synset('german_american.n.01'), Synset('hawaiian.n.02'), Synset('idahoan.n.01'), Synset('illinoisan.n.01'), Synset('indianan.n.01'), Synset('iowan.n.01'), Synset('kansan.n.01'), Synset('kentuckian.n.01'), Synset('louisianan.n.01'), Synset('mainer.n.01'), Synset('marylander.n.01'), Synset('michigander.n.01'), Synset('minnesotan.n.01'), Synset('mississippian.n.02'), Synset('missourian.n.01'), Synset('montanan.n.01'), Synset('nebraskan.n.01'), Synset('nevadan.n.01'), Synset('new_englander.n.01'), Synset('new_hampshirite.n.01'), Synset('new_jerseyan.n.01'), Synset('new_mexican.n.01'), Synset('new_yorker.n.01'), Synset('nisei.n.01'), Synset('north_carolinian.n.01'), Synset('north_dakotan.n.01'), Synset('ohioan.n.01'), Synset('oklahoman.n.01'), Synset('oregonian.n.01'), Synset('pennsylvanian.n.02'), Synset('puerto_rican.n.01'), Synset('rhode_islander.n.01'), Synset('south_carolinian.n.01'), Synset('south_dakotan.n.01'), Synset('southerner.n.01'), Synset('spanish_american.n.01'), Synset('tennessean.n.01'), Synset('texan.n.01'), Synset('tory.n.01'), Synset('utahan.n.01'), Synset('vermonter.n.01'), Synset('virginian.n.01'), Synset('washingtonian.n.01'), Synset('washingtonian.n.02'), Synset('west_virginian.n.01'), Synset('wisconsinite.n.01'), Synset('wyomingite.n.01'), Synset('yankee.n.01'), Synset('yankee.n.03')]\n",
      "Synset('american_english.n.01')\n",
      "[Synset('african_american_vernacular_english.n.01')]\n",
      "Synset('american.n.03')\n",
      "[Synset('creole.n.01'), Synset('latin_american.n.01'), Synset('mesoamerican.n.01'), Synset('north_american.n.01'), Synset('south_american.n.01'), Synset('west_indian.n.01')]\n",
      "Word:  generation\n",
      "Hyponyms: \n",
      "Synset('coevals.n.01')\n",
      "[Synset('peer_group.n.01'), Synset('youth_culture.n.01')]\n",
      "Synset('generation.n.02')\n",
      "[Synset('baby_boom.n.01'), Synset('generation_x.n.01'), Synset('posterity.n.02')]\n",
      "Synset('generation.n.03')\n",
      "[]\n",
      "Synset('generation.n.04')\n",
      "[]\n",
      "Synset('genesis.n.01')\n",
      "[]\n",
      "Synset('generation.n.06')\n",
      "[]\n",
      "Synset('generation.n.07')\n",
      "[Synset('biogenesis.n.02')]\n",
      "Word:  forebears\n",
      "Hyponyms: \n",
      "Synset('forebear.n.01')\n",
      "[Synset('grandparent.n.01'), Synset('great_grandparent.n.01')]\n",
      "Word:  revolution\n",
      "Hyponyms: \n",
      "Synset('revolution.n.01')\n",
      "[Synset('cultural_revolution.n.01'), Synset('green_revolution.n.01')]\n",
      "Synset('revolution.n.02')\n",
      "[Synset('counterrevolution.n.01')]\n",
      "Synset('rotation.n.03')\n",
      "[Synset('axial_rotation.n.01'), Synset('dextrorotation.n.01'), Synset('levorotation.n.01'), Synset('orbital_rotation.n.01'), Synset('spin.n.01')]\n",
      "Word:  committed\n",
      "Hyponyms: \n",
      "Synset('perpetrate.v.01')\n",
      "[Synset('make.v.24'), Synset('recommit.v.01')]\n",
      "Synset('give.v.18')\n",
      "[Synset('apply.v.10'), Synset('rededicate.v.01'), Synset('vow.v.02')]\n",
      "Synset('commit.v.03')\n",
      "[Synset('hospitalize.v.01')]\n",
      "Synset('entrust.v.01')\n",
      "[Synset('commend.v.03'), Synset('consign.v.02'), Synset('obligate.v.02'), Synset('recommit.v.02')]\n",
      "Synset('invest.v.01')\n",
      "[Synset('buy_into.v.01'), Synset('fund.v.04'), Synset('roll_over.v.03'), Synset('shelter.v.02'), Synset('speculate.v.04'), Synset('tie_up.v.02')]\n",
      "Synset('commit.v.06')\n",
      "[]\n",
      "Synset('committed.a.01')\n",
      "[]\n",
      "Synset('attached.a.03')\n",
      "[]\n",
      "Word:  powerful\n",
      "Hyponyms: \n",
      "Synset('powerful.a.01')\n",
      "[]\n",
      "Synset('knock-down.s.01')\n",
      "[]\n",
      "Synset('potent.s.01')\n",
      "[]\n",
      "Synset('brawny.s.01')\n",
      "[]\n",
      "Synset('herculean.s.01')\n",
      "[]\n",
      "Synset('mighty.r.01')\n",
      "[]\n",
      "Word:  supporting\n",
      "Hyponyms: \n",
      "Synset('support.n.08')\n",
      "[Synset('shoring.n.02'), Synset('suspension.n.06')]\n",
      "Synset('support.v.01')\n",
      "[Synset('help.v.01'), Synset('patronize.v.02'), Synset('promote.v.01'), Synset('second.v.01'), Synset('sponsor.v.02'), Synset('undergird.v.01')]\n",
      "Synset('support.v.02')\n",
      "[Synset('fund.v.06'), Synset('provide.v.06'), Synset('see_through.v.01'), Synset('sponsor.v.01'), Synset('subsidize.v.01')]\n",
      "Synset('back.v.01')\n",
      "[Synset('champion.v.01'), Synset('guarantee.v.04')]\n",
      "Synset('hold.v.10')\n",
      "[Synset('block.v.11'), Synset('brace.v.03'), Synset('bracket.v.01'), Synset('buoy.v.02'), Synset('carry.v.05'), Synset('chock.v.02'), Synset('pole.v.02'), Synset('prop_up.v.01'), Synset('scaffold.v.01'), Synset('truss.v.03'), Synset('underpin.v.01')]\n",
      "Synset('confirm.v.01')\n",
      "[Synset('back.v.09'), Synset('document.v.02'), Synset('prove.v.02'), Synset('validate.v.02'), Synset('verify.v.01'), Synset('vouch.v.04')]\n",
      "Synset('subscribe.v.03')\n",
      "[]\n",
      "Synset('corroborate.v.03')\n",
      "[]\n",
      "Synset('defend.v.01')\n",
      "[Synset('apologize.v.02'), Synset('stand_up.v.05'), Synset('uphold.v.02')]\n",
      "Synset('support.v.09')\n",
      "[]\n",
      "Synset('patronize.v.04')\n",
      "[]\n",
      "Synset('digest.v.03')\n",
      "[Synset('accept.v.07'), Synset('bear_up.v.01'), Synset('pay.v.09'), Synset('sit_out.v.02'), Synset('stand_for.v.04'), Synset('take_a_joke.v.01'), Synset('take_lying_down.v.01')]\n",
      "Synset('encouraging.s.02')\n",
      "[]\n",
      "Synset('load-bearing.s.01')\n",
      "[]\n",
      "Word:  themselves\n",
      "Hyponyms: \n"
     ]
    }
   ],
   "source": [
    "np_hyp = []\n",
    "\n",
    "# Loop trough distribution\n",
    "for i in f_dist.most_common(10):\n",
    "    # Print word\n",
    "    print ('Word: ', i[0])\n",
    "    count=0\n",
    "    \n",
    "    # Print Hyponyms\n",
    "    print ('Hyponyms: ')\n",
    "    for j in wn.synsets(i[0]):\n",
    "        print (j)\n",
    "        print (j.hyponyms())\n",
    "        count+= len(j.hyponyms())\n",
    "    np_hyp.append([i[0], count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_1 = []\n",
    "np_2 = []\n",
    "\n",
    "# Get synonyms\n",
    "for i in range(10):\n",
    "    np_1.append(np_hyp[i][0])\n",
    "\n",
    "for j in range(10):\n",
    "    np_2.append(np_hyp[j][1])\n",
    "\n",
    "# Create dataframe\n",
    "df_hyp = pd.DataFrame({'Top 10 Hyponyms':np_1, 'n':np_2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F.List all hyponyms of the 10 most frequently used words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Top 10 Hyponyms</th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>americans</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>supporting</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>committed</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>revolution</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>citizens</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>generation</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>president</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>forebears</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>powerful</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>themselves</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Top 10 Hyponyms   n\n",
       "2       americans  75\n",
       "8      supporting  42\n",
       "6       committed  16\n",
       "5      revolution   8\n",
       "0        citizens   7\n",
       "3      generation   6\n",
       "1       president   3\n",
       "4       forebears   2\n",
       "7        powerful   0\n",
       "9      themselves   0"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hyp.sort_values(by=\"n\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "G.Reflect on the results.\n",
    "\n",
    "The word with highest number of synonyms is \"Supporting\". The meaning of support is to promote interest or cause of any activity. There are many words with that type of meaning. The word \"Themselves\" and \"citizens\" have the least synonyms as they relate to a particular type with not much of diversity. A word whose meaning is included in the meaning of another more general word is known as Hyponyms. The word \" Americans have the most number of hyponyms as the americans have many types and its meaning is related to many general word. The word \"Themselves\" and \"Powerful\" has no hyponyms as they are in their general form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
